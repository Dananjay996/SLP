{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SreeDananjay S(21BAI1807)\n",
    "#### Speech and Language processing Lab assignment 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/dananjay/.local/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in /home/dananjay/.local/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: numpy in /home/dananjay/.local/lib/python3.10/site-packages (1.25.2)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/dananjay/.local/lib/python3.10/site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/dananjay/.local/lib/python3.10/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/dananjay/.local/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/dananjay/.local/lib/python3.10/site-packages (from librosa) (4.9.0)\n",
      "Collecting lazy-loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/lib/python3/dist-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dananjay/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dananjay/.local/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dananjay/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dananjay/.local/lib/python3.10/site-packages (from matplotlib) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/dananjay/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dananjay/.local/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/dananjay/.local/lib/python3.10/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dananjay/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/dananjay/.local/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/dananjay/.local/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/dananjay/.local/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/dananjay/.local/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dananjay/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dananjay/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dananjay/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dananjay/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2023.11.17)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: soxr, llvmlite, lazy-loader, audioread, soundfile, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.43.0 numba-0.60.0 pooch-1.8.2 soundfile-0.12.1 soxr-0.5.0.post1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa scikit-learn matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env/python\n",
    "#\n",
    "# A script to download bird sound files from the www.xeno-canto.org archives with metadata\n",
    "# The program downloads all the files found with the search terms into\n",
    "# subdirectory data/xeno-canto-dataset/searchTerm.\n",
    "# and corresponding json files.\n",
    "\n",
    "# Karoliina Oksanen, 2014\n",
    "# Updated to python 3.7.4, Agnieszka Mikolajczyk, 2019\n",
    "# Added json download, and extraction of metadata from json, Agnieszka Mikolajczyk 2019\n",
    "\n",
    "import urllib.request, json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# returns the Xeno Canto catalogue numbers for the given list of search terms.\n",
    "# @param searchTerms: list of search terms\n",
    "# http://www.xeno-canto.org/explore?query=common+snipe\n",
    "\n",
    "\n",
    "# Creates the subdirectory data/xeno-canto-dataset if necessary\n",
    "# Downloads and saves json files for number of pages in a query\n",
    "# and directory path to saved json's\n",
    "def save_json(searchTerms, birdName, country):\n",
    "    numPages = 1\n",
    "    page = 1\n",
    "    # create a path to save json files and recordings\n",
    "    path = \"../data/xeno-canto-dataset/\" + birdName.replace(':', '') + \"/\" + country\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Creating subdirectory \" + path + \" for downloaded files...\")\n",
    "        os.makedirs(path)\n",
    "        # download a json file for every page found in a query\n",
    "    while page < numPages + 1:\n",
    "        print(\"Loading page \" + str(page) + \"...\")\n",
    "        url = 'https://www.xeno-canto.org/api/2/recordings?query={0}&page={1}'.format(searchTerms.replace(' ', '%20'),\n",
    "                                                                                      page)\n",
    "        print(url)\n",
    "        jsonPage = urllib.request.urlopen(url)\n",
    "        jsondata = json.loads(jsonPage.read().decode('utf-8'))\n",
    "        filename = path + \"/jsondata_p\" + str(page) + \".json\"\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(jsondata, outfile)\n",
    "        # check number of pages\n",
    "        numPages = jsondata['numPages']\n",
    "        page = page + 1\n",
    "    print(\"Found \", numPages, \" pages in total.\")\n",
    "    # return number of files in json\n",
    "    # each page contains 500 results, the last page can have less than 500 records\n",
    "    print(\"Saved json for \", (numPages - 1) * 500 + len(jsondata['recordings']), \" files\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# reads the json and return the list of values for selected json part\n",
    "# i.e. \"id\" - ID number, \"type\": type of the bird sound such as call or song\n",
    "# for all Xeno Canto files found with the given search terms.\n",
    "def read_data(searchTerm, path):\n",
    "    data = []\n",
    "    numPages = 1\n",
    "    page = 1\n",
    "    # read all pages and save results in a list\n",
    "    while page < numPages + 1:\n",
    "        # read file\n",
    "        with open(path + \"/jsondata_p\" + str(page) + \".json\", 'r') as jsonfile:\n",
    "            jsondata = jsonfile.read()\n",
    "        jsondata = json.loads(jsondata)\n",
    "        # check number of pages\n",
    "        numPages = jsondata['numPages']\n",
    "        # find \"recordings\" in a json and save a list with a search term\n",
    "        for k in range(len(jsondata['recordings'])):\n",
    "            data.append(jsondata[\"recordings\"][k][searchTerm])\n",
    "        page = page + 1\n",
    "    return data\n",
    "\n",
    "\n",
    "# downloads all sound files found with the search terms into xeno-canto directory\n",
    "# into catalogue named after the search term (i.e. Apus apus)\n",
    "# filename have two parts: the name of the bird in latin and ID number\n",
    "def download(searchTerms, birdName, country):\n",
    "    # create data/xeno-canto-dataset directory\n",
    "    path = save_json(searchTerms, birdName, country)\n",
    "    # get filenames: recording ID and bird name in latin from json\n",
    "    filenamesID = read_data('id', path)\n",
    "    filenamesCountry = read_data('cnt', path)\n",
    "    # get website recording http download address from json\n",
    "    fileaddress = read_data('file', path)\n",
    "    numfiles = len(filenamesID)\n",
    "    print(\"A total of \", numfiles, \" files will be downloaded\")\n",
    "    for i in range(0, numfiles):\n",
    "        print(\"Saving file \", i + 1, \"/\", numfiles,\n",
    "              \": data/xeno-canto-dataset/\" + birdName.replace(':', '') + filenamesID[\n",
    "                  i] + \".mp3\")\n",
    "        # Check if fileaddress[i] starts with 'http' or 'https'. If not, prefix the base URL.\n",
    "        if not fileaddress[i].startswith(\"http\"):\n",
    "            full_url = \"https://www.xeno-canto.org\" + fileaddress[i]\n",
    "        else:\n",
    "            full_url = fileaddress[i]\n",
    "\n",
    "        urllib.request.urlretrieve(full_url, path + \"/\" + birdName + filenamesID[i] + \".mp3\")\n",
    "\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    # download all sounds type song, from bird list recorded in selected countries \n",
    "    countries = ['Poland', 'Germany', 'Slovakia', 'Czech', 'Lithuania']\n",
    "    birds = ['Emberiza Citrinella',\n",
    "             'Parus Major',\n",
    "             'Phylloscopus Collybita',\n",
    "             'Sylvia Atricapilla',\n",
    "             'Acrocephalus Arundinaceus']\n",
    "    for country in countries:\n",
    "        for bird in birds:\n",
    "            download(bird + ' cnt:' + country + ' type:song', bird.replace(' ', ''), country)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def extract_mfcc_from_wav(file_path, n_mfcc=13):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "def generate_spectrogram(file_path, output_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def extract_mfcc_from_spectrogram(spectrogram_path, n_mfcc=13):\n",
    "    # Load the spectrogram as grayscale\n",
    "    img = plt.imread(spectrogram_path)\n",
    "    # Convert to grayscale if needed\n",
    "    if len(img.shape) == 3:\n",
    "        img = np.mean(img, axis=2)\n",
    "    return img.flatten()[:n_mfcc]\n",
    "\n",
    "def prepare_dataset(file_paths, labels):\n",
    "    features = []\n",
    "    for file_path in file_paths:\n",
    "        mfcc_features = extract_mfcc_from_wav(file_path)\n",
    "        features.append(mfcc_features)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "file_paths = [\"bird1.wav\", \"bird2.wav\", \"bird3.wav\"]\n",
    "labels = [0, 1, 1] \n",
    "\n",
    "# Prepare dataset\n",
    "X, y = prepare_dataset(file_paths, labels)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Classification using Support Vector Machine (SVM)\n",
    "classifier = SVC(kernel='rbf')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
